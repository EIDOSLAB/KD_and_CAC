{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b # weight * x + bias => line\n",
    "\n",
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([4517.2969,   82.6000])\n"
     ]
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True) # requires_grad propagate to every tensor generated from this, with all history of executed operations\n",
    "\n",
    "print(params.grad)\n",
    "\n",
    "loss = loss_fn(model(t_u, *params), t_c)\n",
    "loss.backward() # compute gradients (if applied operations are differentiable)\n",
    "\n",
    "print(params.grad)\n",
    "\n",
    "if params.grad is not None:\n",
    "    params.grad.zero_() # grad accumulate at each backward call, instead we usually want to calculate new derivative at each iteration, so clean-up after using it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(n_epochs):\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "        \n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            params -= learning_rate * params.grad\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print(f'Epoch {epoch}, Loss {float(loss)}')\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 80.36434173583984\n",
      "Epoch 500, Loss 7.843369007110596\n",
      "Epoch 1000, Loss 3.8254828453063965\n",
      "Epoch 1500, Loss 3.091630458831787\n",
      "Epoch 2000, Loss 2.9575960636138916\n",
      "Epoch 2500, Loss 2.9331159591674805\n",
      "Epoch 3000, Loss 2.9286458492279053\n",
      "Epoch 3500, Loss 2.9278290271759033\n",
      "Epoch 4000, Loss 2.9276793003082275\n",
      "Epoch 4500, Loss 2.927651882171631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_un = t_u * 0.1\n",
    "\n",
    "training_loop(n_epochs=5000,\n",
    "              learning_rate=1e-2,\n",
    "              params=torch.tensor([1.0, 0.0], requires_grad=True),\n",
    "              t_u=t_un,\n",
    "              t_c=t_c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02d069b80ce8d1405797661061ea8b136c47b884583d5712563c1fdf85261dc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
