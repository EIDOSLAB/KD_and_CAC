\chapter{Conclusion}\label{sec:conclusion}
Early detection of calcium in the coronary arteries allows to identify atherosclerosis in the initial stage of its development and therefore it is fundamental to reduce the risk for adverse cardiovascular events.
CAC estimation is today performed analysing CT scans obtained using a relatively expensive device, that exposes the patient to a not negligible amount of ionizing radiations, generally not recommended for low-risk patients.
Diagnosis of atherosclerosis could be really improved if a reliable way to detect CAC was found using a less dangerous and cheaper exam than CT scans, such as CXRs.

Detect CAC on CXRs is a really hard task for a medical expert, if even possible.
Some attempts to use artificial neural networks to accomplish this task can be found in literature, but none of them reached results satisfactory enough to be applied in clinical practice.

With this work, we explored possibility to use multi-modal knowledge distillation to train a neural network to detect CAC on CXRs exploiting knowledge learned by a teacher model that can resolve the same task on CT scans.
We've been able to slightly improve performance of \emph{SimpleCXR} model with some knowledge distillation experiments and we got sometimes close to state of the art results by individually improving some of the evaluation metrics, but we didn't reach or surpass those results.

The task we tried to resolve is inherently complex for several factors, including: biases in the data and difficulty of the task even for human experts; indeed, it is still not clear if it is possible to achieve satisfactory results in general.
Our dataset is exceptional in the sense that provides paired high quality images of each patient with manual annotations, which requires long work and involves many experts; however, its dimension is quite limited considering that traditional training of neural networks requires several orders of magnitude more samples.
For this reason, techniques like transfer learning and KD are required in order to improve training performance.

Our results are not dissimilar to what obtained by \citeauthor{zhang2023distilling} \cite{zhang2023distilling}: they successfully applied KD in a multi-modal scenario, but in their experiments KD alone showed worse results than those obtained without KD and were beneficial only combined with a specific autoencoder and fine tuning.
It is possible that also on our case KD alone is not enough, or that it would need to be improved by different combinations of distillation techniques and network architectures.
KD is an emerging technique and in future it could be possible to improve our results with a deeper understanding of the teacher and student relation needed to achieve optimal results.

We observed that our models were generally weaker in sensitivity, suggesting that a better balance of class distribution or methods that take into account this bias in data, like usage of a balanced loss, could improve results.
Another possible improvement could be to increase the available amount of data with particular attention to the low, intermediate and high risk classes that are currently underrepresented, enabling possibility of classification using all five risk categories and probably improving also KD that usually takes benefit of similarity between different classes.
